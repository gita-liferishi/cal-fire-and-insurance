{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required dependencies\n",
        "!pip install camelot-py[base]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKbbq0RpxSR9",
        "outputId": "98587248-6395-49fc-b879-3ecfe051c3aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting camelot-py[base]\n",
            "  Downloading camelot_py-1.0.9-py3-none-any.whl.metadata (9.8 kB)\n",
            "\u001b[33mWARNING: camelot-py 1.0.9 does not provide the extra 'base'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from camelot-py[base]) (8.3.1)\n",
            "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[base]) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.26.1 in /usr/local/lib/python3.12/dist-packages (from camelot-py[base]) (2.0.2)\n",
            "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[base]) (3.1.5)\n",
            "Collecting pdfminer-six>=20240706 (from camelot-py[base])\n",
            "  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdf<6.0,>=4.0 (from camelot-py[base])\n",
            "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.12/dist-packages (from camelot-py[base]) (2.2.2)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[base]) (0.9.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.12/dist-packages (from camelot-py[base]) (4.12.0.88)\n",
            "Collecting pypdfium2>=4 (from camelot-py[base])\n",
            "  Downloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[base]) (11.3.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl>=3.1.0->camelot-py[base]) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->camelot-py[base]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->camelot-py[base]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->camelot-py[base]) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20240706->camelot-py[base]) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20240706->camelot-py[base]) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[base]) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py[base]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[base]) (2.23)\n",
            "Downloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading camelot_py-1.0.9-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pypdf, pdfminer-six, camelot-py\n",
            "Successfully installed camelot-py-1.0.9 pdfminer-six-20251107 pypdf-5.9.0 pypdfium2-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Kvlygt1okLFF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import camelot\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "warnings.filterwarnings('ignore')\n",
        "source_dir = \"/content/drive/MyDrive/SI 671 - Final Project Data/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjST9OsDkihY",
        "outputId": "7d370282-9b1e-4d1f-a441-3de1b473fce5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "WGAs8xPIFlHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fire_incidents = pd.read_csv(f\"{source_dir}incidents-ca-fires-other.csv\")\n",
        "print(fire_incidents.columns)\n",
        "print(\"Total Incidents: \", len(fire_incidents))\n",
        "fire_incidents.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "bdHvn2cNkf0A",
        "outputId": "82e9040b-6dd8-4eef-9719-ff1f42d256ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/SI 671 - Final Project Data/incidents-ca-fires-other.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4255918157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfire_incidents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{source_dir}incidents-ca-fires-other.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfire_incidents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total Incidents: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfire_incidents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfire_incidents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/SI 671 - Final Project Data/incidents-ca-fires-other.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read ZCTA file to obtain zip codes\n",
        "zcta = gpd.read_file(f\"{source_dir}tl_2024_us_zcta520/tl_2024_us_zcta520.shp\")\n",
        "\n",
        "ca_zcta = zcta[zcta['ZCTA5CE20'].str.startswith(('90', '91', '92', '93', '94', '95', '96'))]\n",
        "print(f\"Number of CA ZCTAs: {len(ca_zcta)}\")\n",
        "print(f\"Columns: {ca_zcta.columns.tolist()}\")\n",
        "\n",
        "print(f\"\\nMissing latitude: {fire_incidents['incident_latitude'].isna().sum()}\")\n",
        "print(f\"Missing longitude: {fire_incidents['incident_longitude'].isna().sum()}\")"
      ],
      "metadata": {
        "id": "8BasyJHtk--r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fire_incidents.columns"
      ],
      "metadata": {
        "id": "-aRo3o7sOAfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# geometry = [Point(xy) for xy in zip(fire_incidents['incident_longitude'], fire_incidents['incident_latitude'])]\n",
        "# fire_gdf = gpd.GeoDataFrame(fire_incidents, geometry = geometry)\n",
        "\n",
        "fire_gdf = gpd.GeoDataFrame(\n",
        "    fire_incidents,\n",
        "    geometry=gpd.points_from_xy(\n",
        "        fire_incidents['incident_longitude'],\n",
        "        fire_incidents['incident_latitude']\n",
        "    ),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "if fire_gdf.crs != ca_zcta.crs:\n",
        "    fire_gdf = fire_gdf.to_crs(ca_zcta.crs)\n",
        "\n",
        "fires_with_zip = gpd.sjoin_nearest(fire_gdf, ca_zcta, how='left')\n",
        "fires_with_zip = fires_with_zip.rename(columns={'ZCTA5CE20': 'zip_code'})\n",
        "\n",
        "matched = fires_with_zip['zip_code'].notna().sum()\n",
        "unmatched = fires_with_zip['zip_code'].isna().sum()\n",
        "\n",
        "print(f\"Matched to ZIP: {matched} ({matched/len(fires_with_zip)*100:.1f}%)\")\n",
        "print(f\"Not matched: {unmatched} ({unmatched/len(fires_with_zip)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "lWbIRISWmiZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_table_from_pdfs(pdf_path, set_flavor='lattice', cols_to_skip = 4,  sub_categories=5):\n",
        "    try:\n",
        "      print(\"Exttracting data from pdf...\")\n",
        "      pdf_tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor=set_flavor)\n",
        "      print(f\"Dataframe created frome {pdf_path}\")\n",
        "    except Exception as e:\n",
        "      print(f\"{set_flavor} not found - table might be without borders: {e}\")\n",
        "\n",
        "    df_list = []\n",
        "    for table in pdf_tables:\n",
        "        df_list.append(table.df)\n",
        "\n",
        "    df = pd.concat(df_list)\n",
        "    df.columns = df.iloc[1]\n",
        "    df = df.iloc[2:].reset_index(drop=True)\n",
        "    mask = df['Zip'].astype(str).str.upper().isin(['ZIP', 'ZIP_CODE', 'ZIP CODE'])\n",
        "    df = df[~mask]\n",
        "\n",
        "    new_cols = []\n",
        "\n",
        "    for i in range(len(df.columns)):\n",
        "      if i < cols_to_skip:\n",
        "        new_cols.append(df.columns[i])  # First 4 columns unchanged\n",
        "        continue\n",
        "      category_idx = (i - 4) // sub_categories\n",
        "      categories = ['Low', 'Medium', 'High']\n",
        "      category = categories[category_idx]\n",
        "      subcol = df.columns[i]\n",
        "      new_cols.append(f\"{category}_{subcol}\")\n",
        "\n",
        "    df.columns = new_cols\n",
        "\n",
        "    df = df.dropna(how='all')\n",
        "    df = df.reset_index(drop=True)\n",
        "    non_numeric = ['Zip', 'ZIP_Code', 'County', 'Is Distressed Area', 'Region']\n",
        "    for col in df.columns:\n",
        "        if col not in non_numeric:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    df =  df[df['Zip'].str.strip() != '']\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ExlKpH0hG_QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_policy_counts = extract_table_from_pdfs(f\"{source_dir}residential-policy-count.pdf\")\n",
        "res_policy_premium = extract_table_from_pdfs(f\"{source_dir}residential-policy-premium.pdf\")\n",
        "res_policy_exposure = extract_table_from_pdfs(f\"{source_dir}residential-policy-exposure.pdf\")\n",
        "com_policy_counts = extract_table_from_pdfs(f\"{source_dir}commercial-under20-policy-count.pdf\", sub_categories=6)\n",
        "com_policy_premium = extract_table_from_pdfs(f\"{source_dir}commercial-under20-policy-premium.pdf\", sub_categories=6)\n",
        "com_policy_exposure = extract_table_from_pdfs(f\"{source_dir}commercial-under20-policy-exposure.pdf\", sub_categories=6)"
      ],
      "metadata": {
        "id": "7Kq8GoJyHLYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_policy_premium.columns"
      ],
      "metadata": {
        "id": "MuQIwopcN0Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_table_from_growth_pdfs(pdf_path, set_flavor='lattice',policy_metric='Exposure', rows_to_skip=2):\n",
        "  try:\n",
        "      print(\"Exttracting data from pdf...\")\n",
        "      pdf_tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor=set_flavor)\n",
        "      print(f\"Dataframe created frome {pdf_path}\")\n",
        "  except Exception as e:\n",
        "    print(f\"{set_flavor} not found - table might be without borders: {e}\")\n",
        "\n",
        "  df_list = []\n",
        "  for table in pdf_tables:\n",
        "      df_list.append(table.df)\n",
        "\n",
        "  df = pd.concat(df_list)\n",
        "  df.columns = df.iloc[rows_to_skip]\n",
        "  df = df.iloc[rows_to_skip:].reset_index(drop=True)\n",
        "  empty_cols = [col for col in df.columns if col == '']\n",
        "  df = df.drop(columns=empty_cols)\n",
        "\n",
        "  if len(df.columns)==11:\n",
        "    df.columns = ['ZIP_Code', 'Growth_2025', f'Total_{policy_metric}_2025', 'Growth_2024', f'Total_{policy_metric}_2024',\n",
        "               'Growth_2023', f'Total_{policy_metric}_2023', 'Growth_2022', f'Total_{policy_metric}_2022',\n",
        "               'Growth_2021', f'Total_{policy_metric}_2021']\n",
        "  else:\n",
        "    print(\"Inspect columns:\\n\")\n",
        "    print(df.columns)\n",
        "    return\n",
        "\n",
        "  df['ZIP_Code'] = df['ZIP_Code'].astype(str)\n",
        "\n",
        "  for col in df.columns[1:]:\n",
        "    if 'Growth' in col:\n",
        "      # Remove percentage signs and convert to decimal\n",
        "      df[col] = df[col].astype(str).str.replace('%', '', regex=False)\n",
        "    elif 'Exposure' in col:\n",
        "      # Remove dollar signs, newlines, and commas\n",
        "      df[col] = df[col].astype(str).str.replace(r'[\\$\\n,]', '', regex=True)\n",
        "\n",
        "    # Convert to numeric\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "  df =  df[df['ZIP_Code'].str.strip() != '']\n",
        "  df = df.dropna(subset=['ZIP_Code'])\n",
        "  df = df.drop_duplicates(subset=[zip_col], keep='first')\n",
        "  df = df.drop([0, 1]).reset_index(drop=True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "Oy2SmIZOH3v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "com_pif_growth = extract_table_from_growth_pdfs(f\"{source_dir}commercial-pif-yoy-zip.pdf\", set_flavor='stream',policy_metric='PIFs')\n",
        "res_pif_growth = extract_table_from_growth_pdfs(f\"{source_dir}residential-pif-yoy-zip.pdf\",set_flavor='stream',policy_metric='PIFs')\n",
        "com_exp_growth = extract_table_from_growth_pdfs(f\"{source_dir}commercial-exposure-yoy-zip.pdf\",set_flavor='stream',policy_metric='Exposure', rows_to_skip=3)\n",
        "res_exp_growth = extract_table_from_growth_pdfs(f\"{source_dir}residential-exposure-yoy-zip.pdf\", set_flavor='stream',policy_metric='Exposure')"
      ],
      "metadata": {
        "id": "2Hkg19YVPHrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"DIAGNOSING GROWTH DATASET DUPLICATES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check each growth dataset for duplicates\n",
        "for name, df in [\n",
        "    ('res_pif_growth', res_pif_growth),\n",
        "    ('res_exp_growth', res_exp_growth),\n",
        "    ('com_pif_growth', com_pif_growth),\n",
        "    ('com_exp_growth', com_exp_growth)\n",
        "]:\n",
        "    total_rows = len(df)\n",
        "\n",
        "    # Find zip column\n",
        "    zip_col = 'ZIP_Code' if 'ZIP_Code' in df.columns else 'zip_code'\n",
        "\n",
        "    unique_zips = df[zip_col].nunique()\n",
        "    duplicates = total_rows - unique_zips\n",
        "\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Total rows: {total_rows}\")\n",
        "    print(f\"  Unique ZIPs: {unique_zips}\")\n",
        "    print(f\"  Duplicate rows: {duplicates}\")\n",
        "\n",
        "    if duplicates > 0:\n",
        "        # Show which ZIPs are duplicated\n",
        "        dup_counts = df[df.duplicated(subset=[zip_col], keep=False)].groupby(zip_col).size()\n",
        "        print(f\"  Most duplicated ZIPs:\")\n",
        "        print(dup_counts.sort_values(ascending=False).head(10))"
      ],
      "metadata": {
        "id": "KtDwxRng-Pso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fair_datasets = (\n",
        "    res_policy_counts, res_policy_exposure, res_policy_premium, com_policy_counts, com_policy_exposure, com_policy_premium\n",
        "    )\n",
        "growth_datasets = (\n",
        "    com_pif_growth, com_exp_growth, res_pif_growth, res_exp_growth\n",
        ")\n",
        "\n",
        "if 'index_right' in fires_with_zip.columns:\n",
        "    fires_with_zip = fires_with_zip.drop(columns=['index_right'])\n",
        "\n",
        "def create_master_dataset(fires_df, ca_zipcodes,\n",
        "                          fair_datasets, growth_datasets):\n",
        "\n",
        "    master_gdf = ca_zipcodes.copy()\n",
        "    master_gdf['zip_code'] = master_gdf['ZCTA5CE20']\n",
        "\n",
        "    # 2. Add geometric features\n",
        "    master_gdf['centroid'] = master_gdf.geometry.centroid\n",
        "    master_gdf['centroid_lat'] = master_gdf.centroid.y\n",
        "    master_gdf['centroid_lon'] = master_gdf.centroid.x\n",
        "    master_gdf['area_sq_miles'] = master_gdf.geometry.area / 2589988.11  # Convert to sq miles\n",
        "\n",
        "    # 3. Process and join fire data\n",
        "    fire_features = create_fire_features(fires_df, master_gdf)\n",
        "    master_gdf = master_gdf.merge(fire_features, on='zip_code', how='left')\n",
        "\n",
        "    # 4. Join FAIR plan current state\n",
        "    fair_current = merge_fair_current_state(fair_datasets)\n",
        "    master_gdf = master_gdf.merge(fair_current, on='zip_code', how='left')\n",
        "\n",
        "    # 5. Join growth metrics\n",
        "    growth_features = process_growth_metrics(growth_datasets)\n",
        "    master_gdf = master_gdf.merge(growth_features, on='zip_code', how='left')\n",
        "\n",
        "    cols_to_drop = ['ZCTA5CE20', 'GEOID20', 'GEOIDFQ20', 'CLASSFP20', 'MTFCC20', 'FUNCSTAT20', 'ALAND20', 'AWATER20', 'INTPTLAT20', 'INTPTLON20', 'area_sq_miles_y',\n",
        "      'County_exposure', 'Is Distressed Area_exposure', 'Region_exposure', 'County_counts', 'Is Distressed Area_counts', 'Region_counts', 'County_counts',\n",
        "      'Is Distressed Area_counts', 'Region_counts', 'County_premium', 'Is Distressed Area_premium', 'Region_premium', 'County_exposure', 'Is Distressed Area_exposure',\n",
        "      'Region_exposure']\n",
        "\n",
        "    master_gdf = master_gdf.drop(cols_to_drop, axis=1)\n",
        "\n",
        "    cols = ['zip_code'] + [col for col in master_df.columns if col != 'zip_code']\n",
        "    master_gdf = master_gdf[cols]\n",
        "\n",
        "    return master_gdf\n",
        "\n",
        "def create_fire_features(fires_df, zip_gdf):\n",
        "\n",
        "    # Spatial join\n",
        "    fires_zip_tmp = gpd.sjoin(fires_df, zip_gdf[['zip_code', 'geometry']],\n",
        "                                how='left', predicate='within')\n",
        "\n",
        "    if 'zip_code_left' in fires_zip_tmp.columns and 'zip_code_right' in fires_zip_tmp.columns:\n",
        "      # Check how many differ\n",
        "      matches = (fires_zip_tmp['zip_code_left'] == fires_zip_tmp['zip_code_right']).sum()\n",
        "      total = len(fires_zip_tmp)\n",
        "      print(f\"ZIP codes match: {matches}/{total} ({matches/total*100:.1f}%)\")\n",
        "\n",
        "      # See mismatches\n",
        "      mismatches = fires_zip_tmp[\n",
        "          fires_zip_tmp['zip_code_left'] != fires_zip_tmp['zip_code_right']\n",
        "      ][['incident_name', 'zip_code_left', 'zip_code_right', 'geometry', 'incident_latitude', 'incident_longitude']]\n",
        "\n",
        "    if len(mismatches) > 0:\n",
        "        print(f\"\\n{len(mismatches)} fires have different ZIP codes:\")\n",
        "        print(mismatches.head(10))\n",
        "\n",
        "    fires_zip_tmp['zip_code'] = fires_zip_tmp['zip_code_right']\n",
        "\n",
        "    # Drop both old columns\n",
        "    fires_zip_tmp = fires_zip_tmp.drop(columns=['zip_code_left', 'zip_code_right'])\n",
        "\n",
        "    # Add year column\n",
        "    fires_zip_tmp['year'] = pd.to_datetime(\n",
        "        fires_zip_tmp['incident_dateonly_created']\n",
        "    ).dt.year\n",
        "\n",
        "    print(fires_zip_tmp.columns)\n",
        "    # Aggregate by ZIP\n",
        "    fire_agg = fires_zip_tmp.groupby('zip_code').agg({\n",
        "        'incident_id': 'count',\n",
        "        'incident_acres_burned': ['sum', 'mean', 'max'],\n",
        "        'incident_dateonly_created': ['min', 'max'],\n",
        "        'incident_containment': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    fire_agg.columns = ['_'.join(col).strip('_') for col in fire_agg.columns.values]\n",
        "\n",
        "    fire_agg = fire_agg.rename(columns={\n",
        "        'incident_id_count': 'total_fire_incidents',\n",
        "        'incident_acres_burned_sum': 'total_acres_burned',\n",
        "        'incident_acres_burned_mean': 'avg_acres_per_fire',\n",
        "        'incident_acres_burned_max': 'max_acres_burned',\n",
        "        'incident_dateonly_created_min': 'first_fire_date',\n",
        "        'incident_dateonly_created_max': 'last_fire_date',\n",
        "        'incident_containment_mean': 'avg_containment_pct'\n",
        "    })\n",
        "\n",
        "    # Recent activity\n",
        "    current_date = datetime.now()\n",
        "    for years_back in [1, 2, 3 , 4, 5]:\n",
        "        cutoff = current_date - pd.DateOffset(years=years_back)\n",
        "        recent = fires_zip_tmp[\n",
        "            pd.to_datetime(fires_zip_tmp['incident_dateonly_created']) >= cutoff\n",
        "        ]\n",
        "        counts = recent.groupby('zip_code').size().rename(f'fires_last_{years_back}yr')\n",
        "        fire_agg = fire_agg.merge(counts, on='zip_code', how='left')\n",
        "\n",
        "    # Year-by-year counts\n",
        "    yearly = fires_zip_tmp.groupby(['zip_code', 'year']).agg({\n",
        "        'incident_id': 'count',\n",
        "        'incident_acres_burned': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    for year in range(2021, 2026):\n",
        "        year_data = yearly[yearly['year'] == year][['zip_code', 'incident_id']]\n",
        "        year_data = year_data.rename(columns={'incident_id': f'fire_count_{year}'})\n",
        "        fire_agg = fire_agg.merge(year_data, on='zip_code', how='left')\n",
        "\n",
        "        acres_data = yearly[yearly['year'] == year][['zip_code', 'incident_acres_burned']]\n",
        "        acres_data = acres_data.rename(columns={'incident_acres_burned': f'acres_burned_{year}'})\n",
        "        fire_agg = fire_agg.merge(acres_data, on='zip_code', how='left')\n",
        "\n",
        "\n",
        "    # Calculate fire density\n",
        "    fire_agg = fire_agg.merge(\n",
        "        zip_gdf[['zip_code', 'area_sq_miles']],\n",
        "        on='zip_code'\n",
        "    )\n",
        "    fire_agg['fire_density'] = (\n",
        "        fire_agg['total_fire_incidents'] / fire_agg['area_sq_miles']\n",
        "    )\n",
        "\n",
        "    return fire_agg\n",
        "\n",
        "def merge_fair_current_state(fair_datasets):\n",
        "    \"\"\"\n",
        "    Merge all FAIR datasets for current state\n",
        "    \"\"\"\n",
        "    (res_policy_counts, res_policy_exposure, res_policy_premium,\n",
        "     com_policy_counts, com_policy_exposure, com_policy_premium) = fair_datasets\n",
        "\n",
        "    fair_merged = res_policy_counts.merge(res_policy_exposure, on='Zip', how='outer', suffixes=('_counts', '_exposure'))\n",
        "    print(f\"Starting with {len(fair_merged)} unique ZIP codes\")\n",
        "\n",
        "    fair_merged = fair_merged.merge(res_policy_premium, on='Zip', how='outer', suffixes=('', '_premium'))\n",
        "    print(f\"After res_premium: {len(fair_merged)} rows\")\n",
        "\n",
        "    fair_merged = fair_merged.merge(com_policy_counts, on='Zip', how='outer', suffixes=('', '_counts'))\n",
        "    print(f\"After com_counts: {len(fair_merged)} rows\")\n",
        "\n",
        "    fair_merged = fair_merged.merge(com_policy_premium, on='Zip', how='outer', suffixes=('', '_premium'))\n",
        "    print(f\"After com_premium: {len(fair_merged)} rows\")\n",
        "\n",
        "    fair_merged = fair_merged.merge(com_policy_exposure, on='Zip', how='outer', suffixes=('', '_exposure'))\n",
        "    print(f\"After com_exposure: {len(fair_merged)} rows\")\n",
        "\n",
        "    fair_merged = fair_merged.rename(columns={'Zip': 'zip_code'})\n",
        "\n",
        "    return fair_merged\n",
        "\n",
        "def process_growth_metrics(growth_datasets):\n",
        "    \"\"\"\n",
        "    Process year-over-year growth data\n",
        "    \"\"\"\n",
        "    (com_pif_growth, com_exp_growth, res_pif_growth, res_exp_growth) = growth_datasets\n",
        "\n",
        "    growth_merged = res_pif_growth.rename(columns={\n",
        "        'ZIP_Code': 'zip_code',\n",
        "        'Growth_2025': 'res_policy_growth_pct_2024_2025',  # Growth from 2024 to 2025 and similarly\n",
        "        'Total_PIFs_2025': 'res_policy_count_2025',\n",
        "        'Growth_2024': 'res_policy_growth_pct_2023_2024',\n",
        "        'Total_PIFs_2024': 'res_policy_count_2024',\n",
        "        'Growth_2023': 'res_policy_growth_pct_2022_2023',\n",
        "        'Total_PIFs_2023': 'res_policy_count_2023',\n",
        "        'Growth_2022': 'res_policy_growth_pct_2021_2022',\n",
        "        'Total_PIFs_2022': 'res_policy_count_2022',\n",
        "        'Growth_2021': 'res_policy_growth_pct_2020_2021',\n",
        "        'Total_PIFs_2021': 'res_policy_count_2021'\n",
        "    })\n",
        "\n",
        "    # growth_merged = res_pif_growth.copy(\n",
        "    print(f\"After res_policy: {len(growth_merged)} rows, {len(growth_merged.columns)}\")\n",
        "\n",
        "    rename_dict = {}\n",
        "    for col in res_exp_growth.columns:\n",
        "        if col == 'ZIP_Code':\n",
        "            rename_dict[col] = 'zip_code'\n",
        "        elif 'Growth_' in col:\n",
        "            year = col.split('_')[1]\n",
        "            prev_year = str(int(year) - 1)\n",
        "            rename_dict[col] = f'res_exposure_growth_pct_{prev_year}_{year}'\n",
        "        elif 'Total_' in col:\n",
        "            year = col.split('_')[-1]\n",
        "            rename_dict[col] = f'res_exposure_total_{year}'\n",
        "\n",
        "    res_exp_growth = res_exp_growth.rename(columns=rename_dict)\n",
        "\n",
        "    growth_merged = growth_merged.merge(res_exp_growth, on='zip_code', how='outer')\n",
        "    print(f\"After res_exposure: {len(growth_merged)} rows, {len(growth_merged.columns)} columns\")\n",
        "\n",
        "    rename_dict = {}\n",
        "    for col in com_pif_growth.columns:\n",
        "        if col == 'ZIP_Code':\n",
        "            rename_dict[col] = 'zip_code'\n",
        "        elif 'Growth_' in col:\n",
        "            year = col.split('_')[1]\n",
        "            prev_year = str(int(year) - 1)\n",
        "            rename_dict[col] = f'com_policy_growth_pct_{prev_year}_{year}'\n",
        "        elif 'Total_' in col:\n",
        "            year = col.split('_')[-1]\n",
        "            rename_dict[col] = f'com_policy_count_{year}'\n",
        "\n",
        "    com_pif_growth = com_pif_growth.rename(columns=rename_dict)\n",
        "\n",
        "    growth_merged = growth_merged.merge(com_pif_growth, on='zip_code', how='outer')\n",
        "    print(f\"After com_policy: {len(growth_merged)} rows, {len(growth_merged.columns)} columns\")\n",
        "\n",
        "    rename_dict = {}\n",
        "    for col in com_exp_growth.columns:\n",
        "        if col == 'ZIP_Code':\n",
        "            rename_dict[col] = 'zip_code'\n",
        "        elif 'Growth_' in col:\n",
        "            year = col.split('_')[1]\n",
        "            prev_year = str(int(year) - 1)\n",
        "            rename_dict[col] = f'com_exposure_growth_pct_{prev_year}_{year}'\n",
        "        elif 'Total_' in col:\n",
        "            year = col.split('_')[-1]\n",
        "            rename_dict[col] = f'com_exposure_total_{year}'\n",
        "\n",
        "    com_exp_renamed = com_exp_growth.rename(columns=rename_dict)\n",
        "\n",
        "    growth_merged = growth_merged.merge(com_exp_renamed, on='zip_code', how='outer')\n",
        "    print(f\"After com_exposure: {len(growth_merged)} rows, {len(growth_merged.columns)} columns\")\n",
        "\n",
        "    return growth_merged\n",
        "\n",
        "master_df = create_master_dataset(fires_with_zip, ca_zcta, fair_datasets, growth_datasets)"
      ],
      "metadata": {
        "id": "DbejgvrFCnGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_df.tail()"
      ],
      "metadata": {
        "id": "VaU8Gnw4K9ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_df.to_csv(f\"{source_dir}master_combined_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "qyOC9-KoBhc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JyJk7iuOAHNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Analysis"
      ],
      "metadata": {
        "id": "nt0RLLDuFqyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning - Time Series"
      ],
      "metadata": {
        "id": "7D63r9fkFtUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "master_df = pd.read_csv(f\"{source_dir}master_combined_data.csv\")\n",
        "for col in master_df.columns:\n",
        "  print(col)"
      ],
      "metadata": {
        "id": "qd2ASTZKFsjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "709805ec-d05e-468e-cc5c-57528c50fb61"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zip_code\n",
            "geometry\n",
            "centroid\n",
            "centroid_lat\n",
            "centroid_lon\n",
            "area_sq_miles_x\n",
            "total_fire_incidents\n",
            "total_acres_burned\n",
            "avg_acres_per_fire\n",
            "max_acres_burned\n",
            "first_fire_date\n",
            "last_fire_date\n",
            "avg_containment_pct\n",
            "fires_last_1yr\n",
            "fires_last_2yr\n",
            "fires_last_3yr\n",
            "fires_last_4yr\n",
            "fires_last_5yr\n",
            "fire_count_2021\n",
            "acres_burned_2021\n",
            "fire_count_2022\n",
            "acres_burned_2022\n",
            "fire_count_2023\n",
            "acres_burned_2023\n",
            "fire_count_2024\n",
            "acres_burned_2024\n",
            "fire_count_2025\n",
            "acres_burned_2025\n",
            "fire_density\n",
            "Low_Owner-occupied Single Family Residential_counts\n",
            "Low_Tenant-occupied_counts\n",
            "Low_Renters_counts\n",
            "Low_Condo unit-owners_counts\n",
            "Low_Other_counts\n",
            "Low_Other_counts.1\n",
            "Medium_Owner-occupied Single Family Residential_counts\n",
            "Medium_Tenant-occupied_counts\n",
            "Medium_Renters_counts\n",
            "Medium_Condo unit-owners_counts\n",
            "Medium_Other_counts\n",
            "Medium_Other_counts.1\n",
            "High_Owner-occupied Single Family Residential_counts\n",
            "High_Tenant-occupied_counts\n",
            "High_Renters_counts\n",
            "High_Condo unit-owners_counts\n",
            "High_Other_counts\n",
            "High_Other_counts.1\n",
            "Low_Owner-occupied Single Family Residential_exposure\n",
            "Low_Tenant-occupied_exposure\n",
            "Low_Renters_exposure\n",
            "Low_Condo unit-owners_exposure\n",
            "Low_Other_exposure\n",
            "Low_Other_exposure.1\n",
            "Medium_Owner-occupied Single Family Residential_exposure\n",
            "Medium_Tenant-occupied_exposure\n",
            "Medium_Renters_exposure\n",
            "Medium_Condo unit-owners_exposure\n",
            "Medium_Other_exposure\n",
            "Medium_Other_exposure.1\n",
            "High_Owner-occupied Single Family Residential_exposure\n",
            "High_Tenant-occupied_exposure\n",
            "High_Renters_exposure\n",
            "High_Condo unit-owners_exposure\n",
            "High_Other_exposure\n",
            "High_Other_exposure.1\n",
            "County\n",
            "Is Distressed Area\n",
            "Region\n",
            "Low_Owner-occupied Single Family Residential\n",
            "Low_Tenant-occupied\n",
            "Low_Renters\n",
            "Low_Condo unit-owners\n",
            "Low_Other\n",
            "Medium_Owner-occupied Single Family Residential\n",
            "Medium_Tenant-occupied\n",
            "Medium_Renters\n",
            "Medium_Condo unit-owners\n",
            "Medium_Other\n",
            "High_Owner-occupied Single Family Residential\n",
            "High_Tenant-occupied\n",
            "High_Renters\n",
            "High_Condo unit-owners\n",
            "High_Other\n",
            "Low_Commercial Habitational\n",
            "Low_Retail\n",
            "Low_Manufacturing\n",
            "Low_Office Buildings\n",
            "Low_Agricultural\n",
            "Low_Other_counts.2\n",
            "Low_Other_counts.3\n",
            "Medium_Commercial Habitational\n",
            "Medium_Retail\n",
            "Medium_Manufacturing\n",
            "Medium_Office Buildings\n",
            "Medium_Agricultural\n",
            "Medium_Other_counts.2\n",
            "Medium_Other_counts.3\n",
            "High_Commercial Habitational\n",
            "High_Retail\n",
            "High_Manufacturing\n",
            "High_Office Buildings\n",
            "High_Agricultural\n",
            "High_Other_counts.2\n",
            "High_Other_counts.3\n",
            "Low_Commercial Habitational_premium\n",
            "Low_Retail_premium\n",
            "Low_Manufacturing_premium\n",
            "Low_Office Buildings_premium\n",
            "Low_Agricultural_premium\n",
            "Low_Other_premium\n",
            "Medium_Commercial Habitational_premium\n",
            "Medium_Retail_premium\n",
            "Medium_Manufacturing_premium\n",
            "Medium_Office Buildings_premium\n",
            "Medium_Agricultural_premium\n",
            "Medium_Other_premium\n",
            "High_Commercial Habitational_premium\n",
            "High_Retail_premium\n",
            "High_Manufacturing_premium\n",
            "High_Office Buildings_premium\n",
            "High_Agricultural_premium\n",
            "High_Other_premium\n",
            "Low_Commercial Habitational_exposure\n",
            "Low_Retail_exposure\n",
            "Low_Manufacturing_exposure\n",
            "Low_Office Buildings_exposure\n",
            "Low_Agricultural_exposure\n",
            "Low_Other_exposure.2\n",
            "Low_Other_exposure.3\n",
            "Medium_Commercial Habitational_exposure\n",
            "Medium_Retail_exposure\n",
            "Medium_Manufacturing_exposure\n",
            "Medium_Office Buildings_exposure\n",
            "Medium_Agricultural_exposure\n",
            "Medium_Other_exposure.2\n",
            "Medium_Other_exposure.3\n",
            "High_Commercial Habitational_exposure\n",
            "High_Retail_exposure\n",
            "High_Manufacturing_exposure\n",
            "High_Office Buildings_exposure\n",
            "High_Agricultural_exposure\n",
            "High_Other_exposure.2\n",
            "High_Other_exposure.3\n",
            "res_policy_growth_pct_2024_2025\n",
            "res_policy_count_2025\n",
            "res_policy_growth_pct_2023_2024\n",
            "res_policy_count_2024\n",
            "res_policy_growth_pct_2022_2023\n",
            "res_policy_count_2023\n",
            "res_policy_growth_pct_2021_2022\n",
            "res_policy_count_2022\n",
            "res_policy_growth_pct_2020_2021\n",
            "res_policy_count_2021\n",
            "res_exposure_growth_pct_2024_2025\n",
            "res_exposure_total_2025\n",
            "res_exposure_growth_pct_2023_2024\n",
            "res_exposure_total_2024\n",
            "res_exposure_growth_pct_2022_2023\n",
            "res_exposure_total_2023\n",
            "res_exposure_growth_pct_2021_2022\n",
            "res_exposure_total_2022\n",
            "res_exposure_growth_pct_2020_2021\n",
            "res_exposure_total_2021\n",
            "com_policy_growth_pct_2024_2025\n",
            "com_policy_count_2025\n",
            "com_policy_growth_pct_2023_2024\n",
            "com_policy_count_2024\n",
            "com_policy_growth_pct_2022_2023\n",
            "com_policy_count_2023\n",
            "com_policy_growth_pct_2021_2022\n",
            "com_policy_count_2022\n",
            "com_policy_growth_pct_2020_2021\n",
            "com_policy_count_2021\n",
            "com_exposure_growth_pct_2024_2025\n",
            "com_exposure_total_2025\n",
            "com_exposure_growth_pct_2023_2024\n",
            "com_exposure_total_2024\n",
            "com_exposure_growth_pct_2022_2023\n",
            "com_exposure_total_2023\n",
            "com_exposure_growth_pct_2021_2022\n",
            "com_exposure_total_2022\n",
            "com_exposure_growth_pct_2020_2021\n",
            "com_exposure_total_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPIPJh6LKkMr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}